{"title": "Bias_and_Fairness_in_LLMs", "papers": [{"title": "Bias and Fairness in Large Language Models: A Survey", "url": "https://arxiv.org/abs/2309.00770", "txt": "Computer Science > Computation and Language\narXiv:2309.00770 (cs)\nSubmitted on 2 Sep 2023 ([v1](https://arxiv.org/abs/2309.00770v1)), last revised 12 Jul 2024 (this version, v3)\nTitle:Bias and Fairness in Large Language Models: A Survey\nAuthors:Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed\nView a PDF of the paper titled Bias and Fairness in Large Language Models: A Survey, by Isabel O. Gallegos and 8 other authors\nView PDF HTML (experimental)\nAbstract:Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.\nComments: | Accepted at Computational Linguistics, Volume 50, Number 3\n---|---\nSubjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)\nCite as: | arXiv:2309.00770 [cs.CL]\n(or arXiv:2309.00770v3 [cs.CL] for this version)\nFocus to learn more arXiv-issued DOI via DataCite\nSubmission history\nFrom: Isabel Gallegos [[view email]] [[v1]] Sat, 2 Sep 2023 00:32:55 UTC (810 KB) [[v2]] Tue, 12 Mar 2024 00:50:00 UTC (826 KB) [v3] Fri, 12 Jul 2024 20:29:57 UTC (846 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Bias and Fairness in Large Language Models: A Survey, by Isabel O. Gallegos and 8 other authors\n* View PDF\n* HTML (experimental)\n* TeX Source\n* Other Formats\n\nview license \"Rights to this article\"\nCurrent browse context:\ncs.CL\nprev | next >\nnew | recent | 2023-09\nChange to browse by:\ncs cs.AI cs.CY cs.LG\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\n\n1 blog link\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citation Loading...\nBibTeX formatted citation\n√ó\nloading...\nData provided by:\nBookmark\n\"Bookmark on BibSonomy\" \"Bookmark on Reddit\"\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer ([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))\nConnected Papers Toggle\nConnected Papers ([What is Connected Papers?](https://www.connectedpapers.com/about))\nLitmaps Toggle\nLitmaps ([What is Litmaps?](https://www.litmaps.co/))\nscite.ai Toggle\nscite Smart Citations ([What are Smart Citations?](https://www.scite.ai/))\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv ([What is alphaXiv?](https://alphaxiv.org/))\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers ([What is CatalyzeX?](https://www.catalyzex.com))\nDagsHub Toggle\nDagsHub ([What is DagsHub?](https://dagshub.com/))\nGotitPub Toggle\nGotit.pub ([What is GotitPub?](http://gotit.pub/faq))\nHuggingface Toggle\nHugging Face ([What is Huggingface?](https://huggingface.co/huggingface))\nLinks to Code Toggle\nPapers with Code ([What is Papers with Code?](https://paperswithcode.com/))\nScienceCast Toggle\nScienceCast ([What is ScienceCast?](https://sciencecast.org/welcome))\nDemos\nDemos\nReplicate Toggle\nReplicate ([What is Replicate?](https://replicate.com/docs/arxiv/about))\nSpaces Toggle\nHugging Face Spaces ([What is Spaces?](https://huggingface.co/docs/hub/spaces))\nSpaces Toggle\nTXYZ.AI ([What is TXYZ.AI?](https://txyz.ai))\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower ([What are Influence Flowers?](https://influencemap.cmlab.dev/))\nCore recommender toggle\nCORE Recommender ([What is CORE?](https://core.ac.uk/services/recommender))\n* Author\n* Venue\n* Institution\n* Topic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))\n* About\n* Help\n\ncontact arXivClick here to contact arXiv Contact\nsubscribe to arXiv mailingsClick here to subscribe Subscribe\n\n* Copyright\n* Privacy Policy\n\n* Web Accessibility Assistance\n* arXiv Operational Status Get status notifications via email or slack", "similarity": 100}, {"title": "Bias and Fairness in Large Language Models: A Survey", "url": "https://submissions.cljournal.org/index.php/cljournal/article/view/2683", "txt": "Bias and Fairness in Large Language Models: A Survey\nAuthors\nIsabel Orlanes Gallegos Stanford University\nRyan A. Rossi Adobe Research\nJoe Barrow Pattern Data\nMd Mehrab Tanjim Adobe Research\nSungchul Kim Adobe Research\nFranck Dernoncourt Adobe Research\nTong Yu Adobe Research\nRuiyi Zhang Adobe Research\nNesreen K. Ahmed Intel Labs\n\nAbstract\nRapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.\nAuthor Biographies\nIsabel Orlanes Gallegos, Stanford University\nIsabel O. Gallegos is a Ph.D. student in Computer Science at Stanford University. She researches algorithmic fairness to interrogate the role of artificial intelligence in equitable decision-making. Isabel has been awarded two patents and received national computing awards for her work, and is also the recipient of the Hertz Fellowship, NSF Graduate Research Fellowship, Knight-Hennessy scholarship, and GEM Fellowship.\nRyan A. Rossi, Adobe Research\nRyan is a machine learning Senior Research Scientist at Adobe Research. He earned his Ph.D. and M.S. in Computer Science at Purdue University. His research lies in the fields of machine learning and spans theory, algorithms, and applications of large complex relational (network/graph) data from social and physical phenomena.\nJoe Barrow, Pattern Data\nJoe is an NLP Research Scientist at Adobe Research, working out of College Park, MD. He earned his Ph.D. from the University of Maryland. His research interests are improving evaluation and document collection understanding.\nMd Mehrab Tanjim, Adobe Research\nMehrab is a Research Scientist at Adobe Research. He received his Ph.D. and M.Sc. from the Department of Computer Science and Engineering at University of California San Diego, where his research primarily focused on bias and fairness, especially in detecting and mitigating biases in image-generative tasks such as text-to-image, image-to-image translation, text-based image editing, etc. His current research focuses on LLMs and multimodal generative models.\nSungchul Kim, Adobe Research\nSungchul is a Senior Research Scientist at Adobe Research, based in San Jose. He specializes in predictive analytics and data mining, with a particular focus on graph mining across a wide range of real-world applications, including considerations of fairness and bias. Recently, his work has expanded to encompass Large Language Models (LLMs) tailored for enterprise use cases.\nFranck Dernoncourt, Adobe Research\nFranck is an NLP Senior Research Scientist at Adobe Research in Seattle. He received his Ph.D. from MIT. His research interests include neural networks, NLP, and more recently, LLMs. He has published on social bias issues in NLP at EMNLP Findings and LREC.\nTong Yu, Adobe Research\nTong is a Research Scientist at Adobe Research. He received his Ph.D. from the Department of Electrical and Computer Engineering at Carnegie Mellon University. His current research focuses on LLMs, generative models and reinforcement learning, with applications in conversational recommender systems and dialog systems.\nRuiyi Zhang, Adobe Research\nRuiyi is a Research Scientist at Adobe Research. His research interests include (interactive) machine learning, reinforcement learning and NLP, especially the intersection of them. Before that, Ruiyi obtained a Ph.D. from the Department of Computer Science, Duke University.\nNesreen K. Ahmed, Intel Labs\nNesreen is a senior member of the research staff at Intel Labs. She received her Ph.D. from the Computer Science Department at Purdue University. Her research lies in the field of large-scale machine learning and spans the theory and algorithms of graphs, statistical machine learning methods, and their applications in social and information networks.\n\nDownloads\nPresented at ACL 2024\nArticle at MIT Press\n\nPublished\n2024-11-10\nIssue\nVol. 50 No. 3 (2024): Computational Linguistics\nSection\nSurvey article\nMake a Submission\nMake a Submission\nInformation\nFor Readers\nFor Authors\nFor Librarians\n\nAnnouncements\nEMNLP 2025 ‚Äì CL deadlines for Qualifying Papers\nApril 3, 2025\nTo be eligible for presentation (oral or poster, etc.) at EMNLP 2025 , CL papers must satisfy both of the following conditions:\nreceive an accepted decision by July 16 th\nwith the final version submitted (and approved to be sent to MIT Press) by July 30 th\nExclusions :\nYour paper has been presented previously at other conferences.\nYour submission was an extension of prior work.\nYour submission was a survey proposal .\n\nAuthors Registration Fee Details: Author-Registered Papers (for presentation) Industrial/Non-Academic, Academic or Student\nAt least one author of each accepted paper to an ACL conference (ACL, NAACL, EACL, AACL, or EMNLP) must register their paper to present at the conference. Exceptions to the statement above: Accepted Finding that are not being presented. All findings being presented must register their paper. Workshop shared tasked papers do not need to register their paper to present.\nNote all Paper registration fees are based on actual hard cost to the conference - In person registration fees reflects the attendees‚Äô hard costs of food & beverage (breaks, welcome reception and social dinner) along with meeting space, av or poster presentation equipment). Virtual attendees‚Äô registration fees reflect the virtual costs (internet, AV, content management, platforms).\nComputational Linguistics - December 2025 51(1) has been published!\nApril 1, 2025\nCelebrating 50 years!\nBy the end of 2024, the journal Computational Linguistics has reached a significant milestone: It has published exactly 50 volumes over the past half-century. As we launch the first issue of Volume 51, this is an opportune moment to reflect on the journal‚Äôs legacy, ongoing evolution, and the exciting changes that lie ahead. Together, we embark on a journey to open a new chapter for this storied publication.\n\nShow all announcements ...", "similarity": 98}, {"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models", "url": "https://openreview.net/forum?id=IUmj2dw5se", "txt": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models\nSong Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li\nPublished: 22 Jan 2025, Last Modified: 02 Mar 2025\nICLR 2025 Spotlight\nEveryone\nRevisions\nBibTeX\nCC BY 4.0\nKeywords: Fairness, Bias, Benchmark, Large Language Models\nAbstract:\nAs Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.\nPrimary Area: datasets and benchmarks\nCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\nSubmission Guidelines: I certify that this submission complies with the submission instructions as described on the ICLR website.\nAnonymous Url: I certify that there is no URL that could be used to find authors‚Äô identity.\nNo Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.\nSubmission Number: 11458\nLoading", "similarity": 98}, {"title": "Understanding and Mitigating Bias in Large Language Models (LLMs)", "url": "https://www.datacamp.com/blog/understanding-and-mitigating-bias-in-large-language-models-llms", "txt": "Dive into a comprehensive walk-through on understanding bias in LLMs, the impact it causes, and how to mitigate it to ensure trust and fairness.\nContents\n\nUnderstanding LLMs\n\nLLMs Use Cases\n\nThe Mechanism Behind LLMs\n\nThe Prediction and Language Generation Process in LLMs\n\nVersatility in Language Comprehension and Tasks\n\nThe Problem of Bias in LLMs\n\nIdentifying Bias\n\nImpacts of LLM Bias\n\nStrategies for Mitigating LLM Bias\n\nData curation\n\nModel fine-tuning\n\nMultiple methods and metrics for evaluation\n\nLogic in addressing LLM bias\n\nCase Studies and Real-World Applications\n\nGoogle BERT models diverse training data\n\nFairness indicator\n\nOpenAIs pre-training mitigations\n\nReducing Bias While Maintaining Performance\n\nConclusion\n\nIf you‚Äôve been keeping up with the technology world, you‚Äôll have heard the term ‚ÄòLarge Language Models (LLMs)‚Äô being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.\nLLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.\nThis unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.\nUnderstanding LLMs\nLet‚Äôs take it a step back. What are LLMs?\nLLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called ‚Äúlarge‚Äù is because the model requires millions or even billions of parameters, which are used to train the model using a ‚Äòlarge‚Äô corpus of text data.\nLLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.\nIf you are a newbie to the world of LLMs, the following article is recommended to get you up to speed:\nWhat is an LLM? A Guide on Large Language Models and How They Work. Or take our Large Language Models (LLMs) Concepts Course, which is also perfect for learning about LLMs.\nLLMs Use Cases\nLLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.\nLLMs can be used for the following use cases:\nContent creation\nSentiment analysis\nCustomer service\nLanguage translation\nChatbots\nPersonalized marketing\nData analytics\nand more.\n\nThe Mechanism Behind LLMs\nThe Prediction and Language Generation Process in LLMs\nLLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.\nTokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.\n\nThe training phase consists of inputting the model with massive sets of text data to help the model understand various linguistic contexts, nuances, and styles. LLMs will create a knowledge base in which they can effectively mimic the human language.\nVersatility in Language Comprehension and Tasks\nThe versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.\nHowever, the versatility of LLMs goes beyond text prediction. Being able to handle tasks in different languages, different contexts, and different outputs is a type of versatility that is shown in a variety of adaptability applications such as customer service. This is thanks to the extensive training on large specific datasets and the fine-tuning process, which has enhanced its effectiveness in diverse fields.\nHowever, we must remember LLM's unique challenge: bias.\nThe Problem of Bias in LLMs\nAs we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.\nA tool that is known to improve productivity and assist in day-to-day tasks is showing areas of ethical concern. You can learn more about the ethics of AI in our course.\nIdentifying Bias\nThe more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.\nFor example, LLMs can be biased towards genders if the majority of their data shows that women predominantly work as cleaners or nurses, and men are typically engineers or CEOs. The LLM has inherited society's stereotypes due to the training data being fed into it. Another example is racial bias, in which LLMs may reflect certain ethnic groups among stereotypes, as well as cultural bias of overrepresentation to fit the stereotype.\nThe two main origins of biases in LLMs are:\nData sources\nHuman evaluation\n\nAlthough LLMs are very versatile, this challenge shows how the model is less effective when it comes to multicultural content. The concern around LLMs and biases comes down to the use of LLMs in the decision-making process, naturally raising ethical concerns.\nImpacts of LLM Bias\nThe impacts of bias in LLMs affect both the users of the model and the wider society.\nReinforcement of stereotypes\n\nAs we touched on above, there are different types of stereotypes, such as culture and gender. Biases in the training data of LLMs continue to reinforce these harmful stereotypes, causing society to stay in the cycle of prejudice and effectively preventing progress in society.\nIf LLMs continue to digest biased data, they will continue to push cultural division and gender inequality.\nDiscrimination\n\nDiscrimination is the prejudicial treatment of different categories of people based on their sex, ethnicity, age, or disability. Training data can be heavily underrepresented, in which the data does not show a true representation of different groups.\nLLMs outputs that contain biased responses that continue to conserve and maintain racial, gender, and age discrimination aid the negative impact on people's daily lives from marginalized communities, such as the recruitment hiring process to opportunities for education. This leads to a lack of diversity and inclusivity in LLMs outputs, raising ethical concerns as these outputs can be further used for the decision-making process.\nMisinformation and disinformation\n\nIf there are concerns that the training data used for LLMs contain unrepresentative samples or biases, it also raises the question of whether the data contains the correct information. A spread of misinformation or disinformation through LLMs can have consequential effects.\nFor example, in the healthcare department, the use of LLMs that contain biased information can lead to dangerous health decisions. Another example is LLMs containing politically biased data and pushing this narrative that can lead to political disinformation.\nTrust\n\nThe ethical concerns around LLMs are not the main reason why some of society have not taken well to the implementation of AI systems in our everyday lives. Some or many people have concerns about the use of AI systems and how they will impact our society, for example, job loss and economic instability.\nThere is already a lack of trust when it comes to AI systems. Therefore, the bias produced by LLMs can completely diminish any trust or confidence that society has in AI systems overall. In order for LLM technology to be confidently accepted, society needs to trust it.\nStrategies for Mitigating LLM Bias\n\nStrategies for Mitigating LLM Bias\nData curation\nLet's start from the beginning, the data involved. Companies need to be highly responsible for the type of data that they input into models.\nEnsuring that the training data used for LLMs has been curated from a diverse range of data sources. Text datasets that have come from different demographics, languages, and cultures will balance the representation of the human language. This ensures that the training data does not contain unrepresentative samples and guides targeted model fine-tuning efforts, which can reduce the impact of bias when used by the wider community.\nModel fine-tuning\nOnce a range of data sources has been collated and inputted into the model, organizations can continue to improve accuracy and reduce biases through model fine-tuning. There are several fine-tuning approaches, such as:\nTransfer Learning: This process involves using a pre-trained model and training further on it using a smaller and more specific dataset to fine-tune the model output. For example, fine-tuning a model with legal documentation using a general text data pre-trained model.\nBias Reduction Techniques: Organizations should also go the extra mile and implement a bias detection tool into their process to be able to detect and mitigate biases found in the training data. Methods such as counterfactual data augmentation consist of altering the training data to break stereotypical data and reduce gender, racial, or cultural biases in the model.\n\nYou can learn more about the fine-tuning process with our Fine-Tuning LLaMA 2 tutorial, which has a step-by-step guide to adjusting the pre-trained model.\nMultiple methods and metrics for evaluation\nIn order to continuously grow AI systems that can be safely integrated with today's society, organizations need to have multiple methods and metrics used in their evaluation process. Before AI systems such as LLMs are open to the wider community, the correct methods and metrics must be implemented to ensure that the different dimensions of bias are captured in LLM outputs.\nExamples of methods include human evaluation, automatic evaluation, or hybrid evaluation. All of these methods are used to either detect, estimate, or filter biases in LLMs. Examples of metrics include accuracy, sentiment, fairness, and more. These metrics can provide feedback on the bias in LLM outputs and help to continuously improve the biases detected in LLMs.\nIf you would like to learn more about the different evaluations used to improve LLM quality, check out our code-along on Evaluating LLM Responses.\nLogic in addressing LLM bias\nA study from MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) has made significant advancements in LLMs by integrating logical reasoning: Large language models are biased. Can logic help save them?\nThe importance of logical and structured thinking in LLMs allows the models to be able to process and generate outputs with the application of logical reasoning and critical thinking so that LLMs can provide more accurate responses using the reasoning behind them.\nThe process consists of building a neutral language model in which the relationships between tokens are considered ‚Äòneutral‚Äô as there is no logic stating that there is a relationship between the two. CSAIL trained this method on a language model and found the newly trained model was less biased without the need for more data and additional algorithm training.\nLogic-aware language models will have the ability to avoid producing harmful stereotypes.\nCase Studies and Real-World Applications\nGoogle BERT models diverse training data\nGoogle Research continues to improve its LLM BERT by expanding its training data to ensure that it is more inclusive and diverse. The use of large datasets that contain unannotated text for the pre-training phase has allowed the model to later be fine-tuned to adapt to specific tasks. The aim is to create an LLM that is less biased and produces more robust outputs. Google Research has stated that this method has shown a reduction in stereotypical outputs generated by the model and continues to improve its performance in understanding different dialects and cultural contexts.\nFairness indicator\nThe Google Research team has put together several tools called ‚ÄòFairness Indicators,‚Äô which aim to detect bias in machine learning models and go through a mitigating process. These indicators use metrics such as false positives and false negatives to evaluate performance and identify gaps that may be concealed by general metrics.\nOpenAIs pre-training mitigations\nOpenAI has ensured the wider community that safety, privacy, and ethical concerns are at the forefront of their goals. Their pre-training mitigations for DALL-E 2 included filtering out violent and sexual images from the training dataset, removing ‚Äã‚Äãimages that are visually similar to one another, and then teaching the model to mitigate the effects of filtering the dataset.\nReducing Bias While Maintaining Performance\nBeing able to achieve one thing without sacrificing the other can be impossible at times. This applies when trying to achieve a balance between reducing LLM bias while being able to maintain or even improve the model's performance. Debiasing models are imperative to achieve fairness. However, the model's performance and accuracy should not be compromised.\nA strategic approach needs to be implemented to ensure that mitigation methods to reduce bias, such as data curation, model fine-tuning, and the use of multiple methods, do not affect the model's ability to understand and generate language outputs. Improvements need to be made; however, the model's performance should not be a trade-off.\nIt is a matter of trial and error, monitoring and adjustment, debiasing and improvement.\nConclusion\nIn this article, we have covered:\nWhat LLMs are and the mechanism behind them\nThe problem with bias in LLMs and its impact\nHow to mitigate LLM bias\nAlong with real-world examples.\n\nLLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks. Organizations need to understand the lasting negative impact that stereotypes have on individuals and society and use this to ensure that the path to mitigating LLM biases through data curation, model fine-tuning, and logical modelling is established.\nTo learn more about LLMs, check out our Large Language Models Concepts course, which covers how these powerful tools are reshaping the AI landscape.\n\nAuthor\nNisha Arya Ahmed\nA keen learner, seeking to implement my technical data science and strong interpersonal skills, to improve and broaden my tech knowledge and writing skills.\nI transitioned into the world of Data Science from Pharmacology, taking a 9-month bootcamp with Lambda school.\nI am interested in implementing and improving my technical coding and writing skills in Machine Learning and Artificial Intelligence.\nCurrently, I am a Data Scientist and a Freelance Technical Writer.\nTopics\nArtificial Intelligence\n\nNisha Arya AhmedTechnical Writer | Content Creator | Community Manager\nTopics\nArtificial Intelligence\n\nWhat is an LLM? A Guide on Large Language Models and How They Work\nRead this article to discover the basics of large language models, the key technology that is powering the current AI revolution\nJavier Canales Luna\n12 min\nExploring BLOOM: A Comprehensive Guide to the Multilingual Large Language Model\nDive into BLOOM, a multilingual large language model, exploring its creation, technical specs, usage, and ethical aspects for democratizing AI.\nZoumana Keita\n13 min\nSmall Language Models: A Guide With Examples\nLearn about small language models (SLMs), their benefits and applications, and how they compare to large language models (LLMs).\nDr Ana Rojo-Echebur√∫a\n8 min\nInterpretable Machine Learning\nSerg Masis talks about the different challenges affecting model interpretability in machine learning, how bias can produce harmful outcomes in machine learning systems and the different types of technical and non-technical solutions to tackling bias.\nAdel Nehme\n51 min\nFine-Tuning LLMs: A Guide With Examples\nLearn how fine-tuning large language models (LLMs) improves their performance in tasks like language translation, sentiment analysis, and text generation.\nJosep Ferrer\n11 min\nQuantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently\nA Comprehensive Guide to Reducing Model Sizes\nAndrea Valenzuela\n12 min\nSee MoreSee More\nGrow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.", "similarity": 95}, {"title": "How to avoid replicating bias and human error in LLMs", "url": "https://hellofuture.orange.com/en/how-to-avoid-replicating-bias-and-human-error-in-llms/", "txt": "AI systems can replicate human bias. This has become amplified with the rise of large language models. This article explores how to assess and mitigate this bias to allow for fairer systems.\nBias in AISs (Artificial Intelligence Systems) can lead to incorrect decisions or even discrimination that negatively affects individuals and social groups. For example, a recent study reveals that job descriptions created by generative AI convey far more stereotypes than those written by people. Therefore, job descriptions written by GPT-4o contain more bias than the work of a human. These concerns are becoming critical as LLMs (Large Language Models) become exponentially more popular. LLMs, which are used in various fields ranging from virtual assistance to content generation, are increasingly integrated into large-scale applications such as search engines or office suites. This massive adoption by users and businesses underlines the need to understand and manage bias to ensure fair and accountable systems. According to the Stanford 2024 AI Index Report, the number of scientific papers on fairness and bias has increased by 25% since 2022.\n\nLLM bias can appear within the model‚Äôs internal representations as well as in final decisions. Bias in LLMs can be mitigated with or without additional training.\n\n## Regulatory issues\n\nAt the same time, the European Union regulation on artificial intelligence (**AI Act**), which came into force on 1 August 2024 and will begin to apply gradually, aims to regulate the development, marketing and use of AISs. The Act classifies AISs according to their risks, ranging from unacceptable risk to minimal risk. By way of example, any AI-based social scoring system is prohibited, an AIS that assists with recruitment or training tasks is classified as high risk and subject to additional requirements, and limited-risk chatbots are subject only to transparency obligations. ‚ÄúAppropriate‚Äù measures must be put in place for high-risk systems **to detect, prevent and mitigate possible biases** **in the data sets** used to train the models**.** These obligations are particularly important if the biases in question are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations. This means that a high-risk AIS, such as a recruitment or training assistance tool, is subject to these strict bias management requirements. In this article, we will take a closer look at managing LLM bias.\n\n## What is an LLM?\n\nAn LM (Language Model) is a **statistical model designed to represent natural language**. LLMs are advanced versions of these models, trained on vast data sets and using sophisticated architectures. **Their ability to understand and generate text in a coherent and contextually relevant way is revolutionising NLP (Natural Language Processing) applications, improving the performance of machine translation, text generation, sentiment analysis and human-machine interaction systems.**\n\n## What is bias?\n\nA bias can be defined as a **deviation from the norm.** In the field of AI, four broad families of norms have been identified and thus four types of bias: **statistical bias**(e.g. taking an average that simplifies the phenomenon in question),**methodological bias**(e.g. using a device that is not accurate enough to measure the phenomenon in question or using an LLM that has been trained on data sets that are not current), **cognitive bias** (e.g. making a subjective and irrational decision) and **socio-historical bias** (e.g. training an LLM on data sets from a single country and then using it in other countries with different worldviews).\n\n## LLMs are biased\n\nBias in AISs arises from the data sets used to train the model, from the architecture choices and from inappropriate usage. In LLMs, the huge scale of the pre-training data sets, the adaptation process (aligning with human values, specialising in a particular language or field etc.), the bias mitigation choice and the nature of the **Prompt** (e.g. what kind of roleplay) can cause harms of allocation (unjust distribution of resources) or of representation (reinforcement of stereotypes). Whereas harms of allocation produce immediate, easy-to-formalise effects, harms of representation produce long-term effects and are more difficult to formalise.\nThe development pipeline of **Generative LLMs** is built mainly from **two blocks: a basic model**, developed to encode the language and its rules, **and a second model, which is fine-tuned to respond to specific instructions** (e.g. open questions/answers). This second model can then be further tailored to the desired task (e.g. customer relations chatbot) and/or aligned with stated values (e.g. adherence to a company‚Äôs ethical charter).\nThe pipeline of a generative LLM is as follows:\n\nThe biases in LLMs can therefore manifest both in the model itself (**intrinsic bias, which occurs in the model‚Äôs internal representations**) and in the final decisions it takes (**extrinsic bias, which occurs in final decisions and predictions).**\n\n## How to evaluate bias in LLMs\n\nBias in LLMs can be evaluated intrinsically and extrinsically. **Intrinsic methods** include analysing relationships between words in the model‚Äôs internal representations or observing differences in how the model assigns probabilities to words. **Extrinsic methods**, meanwhile, focus on how the model performs at specific tasks. There are **evaluation metrics** specific to traditional NLP tasks that aim to measure the quality of LLMs on reference data sets or generated text. In addition, **benchmarks** (comprising a test data set and NLP task-specific evaluation metrics) can be used to measure the quality and effectiveness of models on specific tasks such as machine translation, text generation etc. However, the test data sets used in the benchmarks can bring their own biases. The suitability of the benchmarks used is therefore a key issue when evaluating LLMs.\n\n## Mitigating bias\n\nBias in LLMs can be mitigated with or without additional training.\n  * **Additional training** may entail supervised or semi-supervised **fine-tuning** or methods to **align the models with expected values** (via reinforcement learning from human feedback or self-diagnostics). These methods make it possible to adjust models to reduce bias while maintaining their performance, but they are resource-intensive and can introduce new opinions, or new representations, and therefore new biases that are consubstantial with the new data added for these mitigation techniques.\n  * **In the absence of additional training**, techniques such as **post-generation self-diagnosis**[1] **or prompt engineering** can be used. These techniques, which do not require additional training, are simpler and require less expertise. They are therefore easier to implement. Post-generation self-diagnosis entails designing specific instructions to steer the model towards generating fairer and less biased responses. This is based on the model‚Äôs ability to evaluate its own outputs after generating them. **Carefully formulated prompts**, meanwhile, can force the model to adopt varied perspectives and avoid stereotypes. **Prompt engineering with roleplay**, where the respondent‚Äôs profile is specified (e.g. gender, education level), is also an effective method for revealing and mitigating certain biases. For example, by asking the model to answer a question as if it were an expert or a person from a specific social group, the bias implicit in the generated responses can be identified and corrected.\n\nVigilance and compromise are sometimes necessary to ensure that bias-mitigation actions do not harm the model‚Äôs performance.\n\n## Corporate action plan\n\nAll the above **must be combined with appropriate governance,** including establishing guidelines for managing bias, appointing ethics officers and managers responsible for training employees, and implementing active monitoring and development of technical tools. It is vital that the members of the teams involved in developing AISs are diverse.\n**Furthermore, several actions must be carried out** on a case-by-case basis, depending on the use case, the deployment context and the people involved**.**\n**First,** it is necessary to**choose the fairness criteria and**[**metrics**] and then to identify the NLP task underlying the use case and the known biases**. Then**,**different LLMs should be compared using an existing**[**benchmark**] or developing one ad hoc. Developing a benchmark requires collecting data, annotating it if necessary, defining the task and establishing how to measure that the task has been performed well on the body of data in question (evaluation metrics). **Then** it is a question of**selecting the least biased (and sufficiently powerful) model** by comparing the results of different models on the benchmark used. **The last step** uses prompt engineering to compare the results obtained on the benchmark with different prompts **in an attempt to optimise the prompt**, in accordance with the chosen fairness criteria. It should be noted that model performance may vary according to the parameters of the prompt (temperature[2], instruction[3], context[4], whether or not examples are provided[5] and system prompt[6]) and the role taken in the prompt. Testing and adjusting models according to specific use cases is therefore essential.\n\n## Conclusion\n\nTo sum up, managing LLM bias remains a **complex and evolving subject**. Although there are ways to evaluate (intrinsic and extrinsic methods, existing or new benchmark) and mitigate bias (with or without additional training), the area is not yet fully mature. **Within companies,** the focus is currently on **organisation,** **developing prototypes and experimenting**. Developing fair and accountable LLMs will undoubtedly help with their large-scale adoption.\n[1] This technique is based on the model‚Äôs ability to evaluate its own outputs after generating them. The model uses predefined criteria to evaluate the quality and fairness of the generated output (e.g. give me the answer and tell me if it is biased). These criteria may include measures of bias, stereotyping or toxicity. If the initial output is deemed to be biased or inappropriate, the model can either adjust the output in real time or generate a new response that takes the evaluation criteria into account. The revised output is then validated to ensure that it meets the fairness and quality criteria before being presented to the user.\n[2] Adjusting the temperature of the model allows you to control the creativity and diversity of the responses generated. Temperature is a hyperparameter that controls the probability of selecting subsequent words in a sequence. A low temperature (close to 0) makes the model more deterministic, while a high temperature (close to 1) increases the diversity of responses.\n[3] Instructions provide clear guidance on the task to be performed. They should be written concisely and accurately to minimise ambiguity and guide the model towards a specific response.\n[4] Context can be added to the prompt to provide general information that makes the instruction easier to understand. The context should be positioned at the beginning of the prompt in order to place the model in a specific framework and guide its responses.\n[5] Providing one or more examples of the expected prediction can help the model understand the task. The example must be representative of the task and be positioned at the end of the prompt to maximise its impact.\n[6] This involves adding guidelines to limit model responses to a specific format, thereby reducing noise and irrelevant responses.\n## Read more :\n\n### Lexicon\nLexicon\nLexicon\n**NLP (Natural Language Processing)**\nAn area of artificial intelligence that focuses on how machines understand and manipulate human language. It encompasses tasks such as machine translation, voice recognition, sentiment analysis, text generation etc.\n**Prompt**\nA short sentence or text provided as an input to a language model to guide it in text generation. It can be used to specify the subject, style or constraints of the text to be generated. The prompt can influence the content and structure of the text generated by the model.\n**Generative LLMs**\nA specific type of language model capable of generating text autonomously. When you train an LLM like Llama, Bard or GPT-4, you teach it how to generate text based on a wide range of existing examples.\n**benchmarks**\na benchmark is an ensemble comprising a body of data, tasks and ways of measuring that the task has been performed well on the body of data in question (evaluation metrics). Benchmarks measure the quality and effectiveness of models on specific tasks such as machine translation, text generation, sentiment analysis etc. They allow researchers and developers to determine how one model behaves versus other models based on objective and reproducible criteria.\n### Authors\nChrist√®le Tarnec\nResearch Engineer\n\nAnais Bekolo\nComputational linguist\n\nEmilie Sirvent-Hien\nResponsible AI Program Manager\n\n## Read also on Hello Future\n\nResearch | Blog\n### Understanding the general public‚Äôs perception of online risks: beyond official definitions\nDiscover\n\nResearch | Blog\n### NORIA: Network anomaly detection using knowledge graphs\nDiscover\n\nResearch | Article\n### Money laundering: a novel approach with new algorithms to combat smurfing\nDiscover\n\nResearch | Article\n### Limiting the Carbon Footprint of AI\nDiscover\n\nResearch | Blog\n### More Sustainable Urban Logistics Using Digital Twins\nDiscover\n\nResearch | Blog\n### Machine-Learning-Based Early Decision-Making (ML-EDM)\nDiscover\n\nResearch | Article\n### Machine learning to combat ocean plastic pollution\nDiscover\n\nResearch | Article\n### Green and Local Energy Exchanges and Optimizing Network Consumption\nDiscover\n1\n2\n3\n4\n5\n6\n7\n\nPrevious Next\nFollow us\nTwitter\nFacebook\nInstagram\nYouTube\nLinkedin\n\nGoogle Play Store\nApp Store\n\nKeywordsKeywords\n5G\nConnectivity\nCybersecurity\nDevice\nDigital\nIndustry\nMachine learning\nSmart city\nSociety\n\nOther sitesOther sites\nGroup‚Äôs websites\n\nNewsletterNewsletter\nSign up to our newsletter\nUnsubscribe from the newsletter\nNewsletter archive\n\nWho are we? Contact Accessibility Legal Notice Personal Data Cookies ¬© Orange 2024\nSubscribe to our newsletter", "similarity": 95}, {"title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications", "url": "https://aclanthology.org/2024.naacl-long.198/", "txt": "Correct Metadata for\nImportant : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>.\nAuthors Adjust author names and order to match the PDF.Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>.\nVerification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .)\n\nAuthors concatenated from the text boxes above:\nALL author names match the snapshot above‚Äîincluding middle initials, hyphens, and accents.\nSubmit\nAbstract\nRecent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.\nAnthology ID:\n    2024.naacl-long.198\nVolume:\n    Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\nMonth:\n    June\nYear:\n    2024\nAddress:\n    Mexico City, Mexico\nEditors:\n    Kevin Duh, Helena Gomez, Steven Bethard\nVenue:\n    NAACL\nSIG:\nPublisher:\n    Association for Computational Linguistics\nNote:\nPages:\n    3603‚Äì3620\nLanguage:\nDOI:\n    10.18653/v1/2024.naacl-long.198 To the current version of the paper by DOI\nBibkey:\n    liu-etal-2024-confronting\nCite (ACL):\n    Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. 2024. Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 3603‚Äì3620, Mexico City, Mexico. Association for Computational Linguistics.\nCite (Informal):\n    Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications (Liu et al., NAACL 2024)\nCopy Citation:\n    BibTeX Markdown MODS XML Endnote More options‚Ä¶\n\nPDF\n\nCite\n\nSearch\n\nVideo\n\nFix data\n\nExport citation\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n\n@inproceedings{liu-etal-2024-confronting,\n  title = \"Confronting {LLM}s with Traditional {ML}: Rethinking the Fairness of Large Language Models in Tabular Classifications\",\n  author = \"Liu, Yanchen and\n   Gautam, Srishti and\n   Ma, Jiaqi and\n   Lakkaraju, Himabindu\",\n  editor = \"Duh, Kevin and\n   Gomez, Helena and\n   Bethard, Steven\",\n  booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n  month = jun,\n  year = \"2024\",\n  address = \"Mexico City, Mexico\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2024.naacl-long.198/\",\n  doi = \"10.18653/v1/2024.naacl-long.198\",\n  pages = \"3603--3620\",\n  abstract = \"Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.\"\n}\n\nDownload as File Copy to Clipboard\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<modsCollection xmlns=\"http://www.loc.gov/mods/v3\">\n<mods ID=\"liu-etal-2024-confronting\">\n  <titleInfo>\n    <title>Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications</title>\n  </titleInfo>\n  <name type=\"personal\">\n    <namePart type=\"given\">Yanchen</namePart>\n    <namePart type=\"family\">Liu</namePart>\n    <role>\n      <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm>\n    </role>\n  </name>\n  <name type=\"personal\">\n    <namePart type=\"given\">Srishti</namePart>\n    <namePart type=\"family\">Gautam</namePart>\n    <role>\n      <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm>\n    </role>\n  </name>\n  <name type=\"personal\">\n    <namePart type=\"given\">Jiaqi</namePart>\n    <namePart type=\"family\">Ma</namePart>\n    <role>\n      <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm>\n    </role>\n  </name>\n  <name type=\"personal\">\n    <namePart type=\"given\">Himabindu</namePart>\n    <namePart type=\"family\">Lakkaraju</namePart>\n    <role>\n      <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm>\n    </role>\n  </name>\n  <originInfo>\n    <dateIssued>2024-06</dateIssued>\n  </originInfo>\n  <typeOfResource>text</typeOfResource>\n  <relatedItem type=\"host\">\n    <titleInfo>\n      <title>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</title>\n    </titleInfo>\n    <name type=\"personal\">\n      <namePart type=\"given\">Kevin</namePart>\n      <namePart type=\"family\">Duh</namePart>\n      <role>\n        <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm>\n      </role>\n    </name>\n    <name type=\"personal\">\n      <namePart type=\"given\">Helena</namePart>\n      <namePart type=\"family\">Gomez</namePart>\n      <role>\n        <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm>\n      </role>\n    </name>\n    <name type=\"personal\">\n      <namePart type=\"given\">Steven</namePart>\n      <namePart type=\"family\">Bethard</namePart>\n      <role>\n        <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm>\n      </role>\n    </name>\n    <originInfo>\n      <publisher>Association for Computational Linguistics</publisher>\n      <place>\n        <placeTerm type=\"text\">Mexico City, Mexico</placeTerm>\n      </place>\n    </originInfo>\n    <genre authority=\"marcgt\">conference publication</genre>\n  </relatedItem>\n  <abstract>Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.</abstract>\n  <identifier type=\"citekey\">liu-etal-2024-confronting</identifier>\n  <identifier type=\"doi\">10.18653/v1/2024.naacl-long.198</identifier>\n  <location>\n    <url>https://aclanthology.org/2024.naacl-long.198/</url>\n  </location>\n  <part>\n    <date>2024-06</date>\n    <extent unit=\"page\">\n      <start>3603</start>\n      <end>3620</end>\n    </extent>\n  </part>\n</mods>\n</modsCollection>\n\nDownload as File Copy to Clipboard\n\n%0 Conference Proceedings\n%T Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications\n%A Liu, Yanchen\n%A Gautam, Srishti\n%A Ma, Jiaqi\n%A Lakkaraju, Himabindu\n%Y Duh, Kevin\n%Y Gomez, Helena\n%Y Bethard, Steven\n%S Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\n%D 2024\n%8 June\n%I Association for Computational Linguistics\n%C Mexico City, Mexico\n%F liu-etal-2024-confronting\n%X Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.\n%R 10.18653/v1/2024.naacl-long.198\n%U https://aclanthology.org/2024.naacl-long.198/\n%U https://doi.org/10.18653/v1/2024.naacl-long.198\n%P 3603-3620\n\nDownload as File Copy to Clipboard\nMarkdown (Informal)\nConfronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications (Liu et al., NAACL 2024)\nConfronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications (Liu et al., NAACL 2024)\n\nACL\nYanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. 2024. Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 3603‚Äì3620, Mexico City, Mexico. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard Copy ACL to Clipboard\nACL materials are Copyright ¬© 1963‚Äì2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 25 April 2025 at 11:27 UTC withcommit 2ffb58f .", "similarity": 95}, {"title": "Bias and Fairness in Large Language Models: A Survey", "url": "https://montrealethics.ai/bias-and-fairness-in-large-language-models-a-survey/", "txt": "Montreal AI Ethics Institute\nDemocratizing AI ethics literacy\n# Bias and Fairness in Large Language Models: A Survey\nSeptember 27, 2023\n\nüî¨ Research Summary by ****Isabel O. Gallegos**** , a Ph.D. student in Computer Science at Stanford University, researching algorithmic fairness to interrogate the role of artificial intelligence in equitable decision-making.\n[[Original paper](https://arxiv.org/pdf/2309.00770.pdf>) by Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed]\n**Overview** : Social biases in large language models (LLMs) have been well-documented, but how can we address them? This paper presents a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We consolidate, formalize, and expand notions of social bias and fairness in natural language processing, unify the literature with three intuitive taxonomies, and identify open problems and challenges for future work.\n## **Introduction**\nRapid advancements in large language models (LLMs) have enabled the understanding and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. This paper presents a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing. We then unify the literature by proposing three intuitive taxonomies: two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent bias propagation in LLMs.\n## **Key Insights**\n### The Challenge of Bias in Large Language Models\nThe rise and rapid advancement of large language models (LLMs) has fundamentally changed language technologies. With the ability to generate human-like text and adapt to a wide array of natural language processing (NLP) tasks, the impressive capabilities of these models have initiated a paradigm shift in the development of language models. Instead of training task-specific models on relatively small task-specific datasets, researchers and practitioners can use LLMs as foundation models that can be fine-tuned for particular functions. Even without fine-tuning, foundation models increasingly enable few- or zero-shot capabilities for various scenarios like classification, question-answering, logical reasoning, fact retrieval, information extraction, and more.\nLaying behind these successes, however, is the potential to perpetuate harm. Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already vulnerable and marginalized communities. These harms are ‚Äúsocial bias,‚Äù a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups arising from historical and structural power asymmetries. Though LLMs often reflect existing biases, they can amplify these biases, too; in either case, the automated reproduction of injustice can reinforce systems of inequity.\n### Defining Bias and Fairness for NLP\nDespite the growing emphasis on addressing these issues, bias and fairness research in LLMs often fails to precisely describe the harms of model behaviors: _who_ is harmed, _why_ the behavior is harmful, and _how_ the harm reflects and reinforces social hierarchies. Consolidating literature from machine learning, NLP, and (socio)linguistics, we define several distinct facets of bias to disambiguate the types of social harms that may emerge from LLMs. We organize these harms in a taxonomy of social biases that researchers and practitioners can leverage to accurately describe bias evaluation and mitigation efforts. We shift fairness frameworks typically applied to machine learning classification problems towards NLP and introduce several fairness desiderata that begin to operationalize various fairness notions for LLMs.\n### Taxonomies for Bias Evaluation and Mitigation\nWith the growing recognition of the biases embedded in LLMs has emerged an abundance of works proposing techniques to measure or remove social bias, primarily organized by (1) metrics for bias evaluation, (2) datasets for bias evaluation, and (3) techniques for bias mitigation. We categorize, summarize, and discuss these three research areas.\n#### Metrics for Bias Evaluation\nWe characterize the relationship between evaluation metrics and datasets, which are often conflated in the literature, and we categorize and discuss a wide range of metrics that can evaluate bias at different fundamental levels in a model: (1) _embedding-based_ , which use hidden vector representations; (2) _probability-based_ , which use model-assigned token probabilities; and (3) _generated text-based_ , which use model-generated text continuations\nWe formalize metrics mathematically with a unified notation that improves comparison between metrics. We also identify the limitations of each class of metrics to capture downstream application biases, highlighting areas for future research.\n#### Datasets for Bias Evaluation\nWe categorize datasets by their data structure: (1) _counterfactual inputs_ , or pairs of sentences with perturbed social groups, and (2) _prompts_ or phrases to condition text generation. With this classification, we leverage our taxonomy of metrics to highlight the compatibility of datasets with new metrics beyond those originally posed. We increase comparability between dataset contents by identifying the types of harm and the social groups targeted by each dataset. We highlight consistency, reliability, and validity challenges in existing evaluation datasets as areas for improvement. Finally, we consolidate and share publicly available datasets here: https://github.com/i-gallegos/Fair-LLM-Benchmark\n#### Techniques for Bias Mitigation\nWe classify an extensive range of bias mitigation methods by their intervention stage: (1) _pre-processing_ , which modifies model inputs; (2) _in-training_ , which modifies model parameters via gradient-based updates; (3) _intra-processing_ , which modifies inference behavior without further training; and (4) _post-processing_ , which modifies model outputs. We construct granular subcategories at each mitigation stage to draw similarities and trends between classes of methods, with a mathematical formalization of several techniques with unified notation and representative examples of each class of method. We draw attention to ways that bias may persist at each mitigation stage.\n#### Open Problems and Challenges\nThe work we survey makes important progress in understanding and reducing bias, but several challenges remain largely open. We challenge future research to address power imbalances in LLM development, conceptualize fairness more robustly for NLP, improve bias evaluation principles and standards, expand mitigation efforts, and explore theoretical limits for fairness guarantees.\n## **Between the lines**\nAs LLMs are increasingly deployed and adapted in various applications, bias evaluation and mitigation efforts remain critical research areas to ensure social harms are not automated nor perpetuated by technical systems. However, the role of technical solutions must be contextualized in a broader understanding of historical, structural, and institutional power hierarchies. For instance, who holds power in developing and deploying LLM systems, who is excluded, and how does technical solutionism preserve, enable, and strengthen inequality? We hope our work improves understanding of technical efforts to measure and reduce the perpetuation of bias by LLMs while also challenging researchers to interrogate more deeply the social, cultural, historical, and political contexts that shape the underlying assumptions and values engrained in technical solutions.\nWant quick summaries of the latest research & reporting in AI ethics delivered to your inbox? Subscribe to the AI Ethics Brief. We publish bi-weekly.\n## Primary Sidebar\nüîç SEARCH\nSearch the site ...\n### Spotlight\n#### AI Policy Corner: The Colorado State Deepfakes Act\n#### Special Edition: Honouring the Legacy of Abhishek Gupta (1992‚Äì2024)\n#### AI Policy Corner: The Turkish Artificial Intelligence Law Proposal\n#### From Funding Crisis to AI Misuse: Critical Digital Rights Challenges from RightsCon 2025\n#### The Paris AI Summit: Deregulation, Fear, and Surveillance\n### related posts\n## Evaluating a Methodology for Increasing AI Transparency: A Case Study\n## Experimenting with Zero-Knowledge Proofs of Training\n## Research summary: A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic ...\n## Implications of Distance over Redistricting Maps: Central and Outlier Maps\n## Human-centred mechanism design with Democratic AI\n## Research summary: AI Governance in 2019, A Year in Review: Observations of 50 Global Experts\n## On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models\n## Ethics for People Who Work in Tech\n## Incentivized Symbiosis: A Paradigm for Human-Agent Coevolution\n## De-platforming disinformation: conspiracy theories and their control\n### Partners\nU.S. Artificial Intelligence Safety Institute Consortium (AISIC) at NIST\nPartnership on AI\nThe LF AI & Data Foundation\nThe AI Alliance\n## Footer\n### Categories\n‚Ä¢ Blog ‚Ä¢ Research Summaries ‚Ä¢ Columns ‚Ä¢ Core Principles of Responsible AI ‚Ä¢ Special Topics\n### Signature Content\n‚Ä¢ The State Of AI Ethics\n‚Ä¢ The Living Dictionary\n‚Ä¢ The AI Ethics Brief\n### Learn More\n‚Ä¢ About\n‚Ä¢ Open Access Policy\n‚Ä¢ Contributions Policy\n‚Ä¢ Editorial Stance on AI Tools\n‚Ä¢ Press\n‚Ä¢ Donate\n‚Ä¢ Contact\n### The AI Ethics Brief (bi-weekly newsletter)\n### About Us\nFounded in 2018, the Montreal AI Ethics Institute (MAIEI) is an international non-profit organization equipping citizens concerned about artificial intelligence and its impact on society to take action.\n### Archive\nArchive Select Month April 2025 March 2025 February 2025 January 2025 December 2024 October 2024 June 2024 February 2024 January 2024 December 2023 November 2023 October 2023 September 2023 August 2023 July 2023 June 2023 May 2023 January 2023 December 2022 November 2022 October 2022 August 2022 June 2022 May 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 September 2021 August 2021 July 2021 June 2021 May 2021 April 2021 March 2021 February 2021 January 2021 December 2020 November 2020 October 2020 September 2020 August 2020 July 2020 June 2020 May 2020 April 2020 March 2020 February 2020 December 2019 October 2019 September 2019 August 2019 July 2019 June 2019 May 2019 April 2019 March 2019 February 2019 January 2019 December 2018\n¬© MONTREAL AI ETHICS INSTITUTE. All rights reserved 2024.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLearn more about our open access policy here.\nSave hours of work and stay on top of Responsible AI research and reporting with our bi-weekly email newsletter.\n√ó", "similarity": 95}, {"title": "Navigating The Biases In LLM Generative AI: A Guide To Responsible Implementation", "url": "https://www.forbes.com/councils/forbestechcouncil/2023/09/06/navigating-the-biases-in-llm-generative-ai-a-guide-to-responsible-implementation/", "txt": "The adoption of artificial intelligence (AI) and generative AI, such as ChatGPT, is becoming increasingly widespread. The impact of generative AI is predicted to be significant, offering efficiency and productivity enhancements across industries. However, as we enter a new phase in the technology's lifecycle, it's crucial to understand its limitations before fully integrating it into corporate tech stacks.\n\nLarge language model (LLM) generative AI, a powerful tool for content creation, holds transformative potential. But beneath its capabilities lies a critical concern: the potential biases ingrained within these AI systems. Addressing these biases is paramount to the responsible and equitable implementation of LLM-based technologies.\n\nRecognizing The Biases\n\nPrudent utilization of LLM generative AI demands an understanding of potential biases. Here are several biases that can emerge during the training and deployment of generative AI systems.\n\nMachine bias refers to the biases that are present in the training data used to build LLMs. Since these models learn from vast human-generated datasets, they tend to absorb the biases present in the text, perpetuating stereotypes and discriminations. Biases pertaining to race, gender, ethnicity and socioeconomic status can inadvertently be perpetuated by the AI system, leading to biased outputs.\n\nAvailability bias stems from the fact that LLM generative AI models are exposed to large amounts of publicly available data. As a result, the model is more likely to favor content that is more readily available while neglecting perspectives and information that are less prevalent online.\n\nThe availability bias in an LLM can create information bubbles and echo chambers that simply reinforce existing biases rather than fostering diverse perspectives. It can also lead to misinformation on a given topic if that misinformation is more readily available than factual content. This phenomenon can exacerbate social divisions and undermine the objective and balanced dissemination of knowledge.\n\nConfirmation bias is a psychological tendency in which individuals seek information that confirms their existing beliefs while ignoring evidence that challenges them. This can be demonstrated either in the training data or in the way that the prompt is written to which the generative AI will develop a response.\n\nWhen users seek information on a particular subject, the AI might selectively generate content that reinforces their viewpoints, leading to a reinforcement loop where users only encounter information that confirms their existing biases.\n\nSelection bias emerges when the training data is not representative of the entire population or target audience. If certain groups or perspectives are underrepresented or excluded from the training data, the AI model will lack the necessary knowledge to generate unbiased and comprehensive content.\n\nFor example, if the training data primarily comprises data from Western countries, the AI may struggle to produce accurate and culturally relevant content for non-Western audiences. This omission perpetuates societal inequalities and prevents the AI system from being an inclusive and unbiased information source.\n\nGroup attribution bias emerges when the generative AI attributes specific characteristics or behaviors to an entire group based on the actions of a few individuals. For example, the AI might associate negative attributes with specific ethnicities or genders, perpetuating harmful generalizations and prejudices. To avoid this, LLM models must be trained on diverse datasets that reflect the complexities and individuality of different groups.\n\nContextual bias arises when the LLM model struggles to understand or interpret the context of a conversation or prompt accurately. Misunderstanding the context can lead to the generation of inappropriate or misleading responses. Data scientists need to fine-tune the model and carefully curate their prompts to better comprehend the context and avoid generating content that is contextually inappropriate or biased.\n\nLinguistic bias occurs when the LLM generative AI favors certain linguistic styles, vocabularies or cultural references over others. This can result in the AI generating content that is more relatable to certain language groups or cultures while alienating others. Data scientists should work to ensure that the AI model remains linguistically neutral and adapts to various language styles and cultural nuances.\n\nAnchoring bias occurs when an AI model relies too heavily on the initial information it receives. This could lead to the model incorporating early biases present in the training data and perpetuating them throughout its generated content. Data scientists must carefully curate the initial information provided to the model and continuously monitor its outputs to prevent this bias from taking hold.\n\nAutomation bias refers to the tendency of humans to blindly trust AI-generated outputs without critically evaluating them. This is one of the most concerning biases when discussing generative AI, as it causes individuals to place unwarranted trust in AI systems, assuming they are infallible.\n\nWhen relying on LLM generative AI for professional use, it is crucial for data scientists and users to exercise skepticism and independently verify the generated content to avoid propagating false or biased information. Blindly accepting AI-generated content without scrutiny can lead to the dissemination of false or biased information, further amplifying existing biases in society.\n\nMy doctoral study on big data governance provides some guidance for data scientists and technology leaders wanting to harness generative AI from LLMs. The study emphasizes the importance of implementing robust governance mechanisms for big data, which serves as the foundation for these LLM and generative AI models. By creating transparent guidelines for data collection, data scientists can actively identify and minimize biases in the training data.\n\nConclusion\n\nLLM generative AI offers transformative potential across industries, yet biases pose significant risks. Adhering to ethical AI development principles is paramount. Biases built into the models can affect content generation, emphasizing the need for inclusive datasets, robust governance and vigilant evaluation.\n\nTo address these biases, data scientists must curate inclusive and representative training datasets, implement robust governance mechanisms and continuously monitor and audit the AI-generated outputs. Responsible AI deployment safeguards against biases and unlocks AI's true potential in shaping a fair and unbiased technological future.\n\nAs we continue to harness the power of AI, it is essential to exercise caution, promote transparency and strive for fairness to unlock the true potential of these transformative technologies.", "similarity": 95}, {"title": "Fairness standards for large language models", "url": "https://www.oii.ox.ac.uk/research/projects/fairness-standards-for-large-language-models/", "txt": "Fairness standards for large language models\n\nProject Contents\n\nOverview\n\nKey Information\n\nParticipants\n\nOverview\n\nLate 2022 and early 2023 saw breakthroughs in the commercialisation of large language models (LLMs), advanced AI systems designed for natural language processing tasks like text generation, summarisation, and translation. These systems bring about numerous benefits, but also have the potential to exacerbate harmful biases by perpetuating negative stereotypes, erasing marginalised worldviews, and reinforcing political biases.\nThis project explores the role that fairness standards, broadly understood, can play in mitigating harmful biases from LLMs. It seeks to (1) map how standards are being used to mitigate LLM bias; (2) consider the efficacy of, and gaps in, current standardisation efforts; and (3) analyse how these gaps should be filled for societally beneficial outcomes, with a particular focus on the role international standards bodies should play. The project will employ a range of qualitative methods and engage stakeholders from the public, private, and third sectors.\n\nKey Information\n\nFunder:\nInternational Organization for Standardization (ISO)\nProject dates: December 2023 - December 2024\n\nParticipants\n\nHuw Roberts DPhil Student Huw Roberts is a doctoral researcher at the University of Oxford‚Äôs Internet Institute and an Associate Fellow at the Royal United Services Institute (RUSI).View profile Marta Ziosi Former Research Assistant Marta Ziosi‚Äôs research revolves around the use of data-driven risk-assessment tools in the Criminal Justice System. She focuses on alternative methods in machine learning to centre policies around prevention, rather than prediction, of crime.View profile\n\n1 St Giles, Oxford, OX1 3JS, UK +44 (0)1865 287210 General: enquiries@oii.ox.ac.uk Press: press@oii.ox.ac.uk\nStaff Intranet\n\nFOLLOW US:\nLinkedIn link\nBluesky link\nFacebook link\nTwitter link\nYouTube link\nInstagram link\n\nINFORMATION FOR:\nProspective students\nAlumni\nJob seekers\nMedia\nPolicy makers\n\n¬© Oxford Internet Institute 2025 | Terms of Use | Privacy Policy | Cookie Settings | Copyright Policy | Accessibility | Email Webmaster\nWe are using cookies to give you the best experience on our website.\nYou can find out more about which cookies we are using or switch them off in settings.\nAccept Reject Settings\nPrivacy Overview\nStrictly Necessary Cookies\nGoogle Analytics\n\nPrivacy Overview\nThis website uses cookies so that we can provide you with the best user experience possible. Cookie information is stored in your browser and performs functions such as recognising you when you return to our website and helping our team to understand which sections of the website you find most interesting and useful.\nStrictly Necessary Cookies\nmoove_gdrp_popup - a cookie that saves your preferences for cookie settings. Without this cookie, the screen offering you cookie options will appear on every page you visit.\nThis cookie remains on your computer for 365 days, but you can adjust your preferences at any time by clicking on the \"Cookie settings\" link in the website footer.\nPlease note that if you visit the Oxford University website, any cookies you accept there will appear on our site here too, this being a subdomain. To control them, you must change your cookie preferences on the main University website.\nEnable or Disable Cookies Enabled Disabled\nGoogle Analytics\nThis website uses Google Tags and Google Analytics to collect anonymised information such as the number of visitors to the site, and the most popular pages. Keeping these cookies enabled helps the OII improve our website.\nEnabling this option will allow cookies from:\nGoogle Analytics - tracking visits to the ox.ac.uk and oii.ox.ac.uk domains\nThese cookies will remain on your website for 365 days, but you can edit your cookie preferences at any time via the \"Cookie Settings\" button in the website footer.\nEnable or Disable Cookies Enabled Disabled\nPlease enable Strictly Necessary Cookies first so that we can save your preferences!\nEnable All Reject All Save Changes\nPowered by GDPR Cookie Compliance", "similarity": 95}, {"title": "Investigating the Fairness of Large Language Models for Predictions on Tabular Data", "url": "https://openreview.net/forum?id=6jJFmwAlen&noteId=uWWHObGssr", "txt": "Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju\n24 Sept 2023 (modified: 25 Mar 2024)ICLR 2024 Conference Withdrawn SubmissionEveryoneRevisionsBibTeX\nKeywords: Fairness, Social Biases, Large Language Models, In-Context Learning, Tabular Data, Trustworthy ML\nTL;DR: This work explores how LLMs inherit and exhibit social biases, highlighting that these biases are inherent to LLMs, and emphasizing the fairness-related risks associated with utilizing LLMs for predictions on tabular data.\nAbstract:\nRecent literature has suggested the potential of using large language models (LLMs) to make predictions for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in the society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is imperative to explore the following questions: what sources of information do LLMs draw upon when making predictions for tabular tasks; whether and to what extent are LLM predictions for tabular tasks influenced by social biases and stereotypes; and what are the consequential implications for fairness? Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular prediction tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and fine-tuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pre-training corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.\nPrimary Area: societal considerations including fairness, safety, privacy\nCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\nSubmission Guidelines: I certify that this submission complies with the submission instructions as described on.\nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors' identity.\nNo Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.\nSubmission Number: 8922\nLoading", "similarity": 95}, {"title": "Bias and Fairness in Large Language Models: A Survey", "url": "https://github.com/i-gallegos/Fair-LLM-Benchmark", "txt": "Bias and Fairness in Large Language Models: A Survey\n> Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed\nPre-print:\n\nIf you use or discuss our survey in your work, please use the following citation:\n```\n@article{gallegos2023bias,\n  title={Bias and Fairness in Large Language Models: A Survey},\n  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},\n  journal={arXiv preprint arXiv:2309.00770},\n  year={2023}\n}\n\n```\n\nTo enable easy use of bias evaluation datasets, we compile publicly-available ones and provide access here. We provide links to the original data sources below. We do not modify any of the datasets, but do remove unrelated material from the original repositories. Please refer to the original works for more detailed documentation.\nDataset | Link\n---|---\nBBQ |\nBEC-Pro |\nBias NLI |\nBOLD |\nBUG |\nCrowS-Pairs |\nEquity Evaluation Corpus |\nGAP |\nGrep-BiasIR |\nHolisticBias |\nHONEST |\nPANDA |\nRealToxicityPrompts |\nRedditBias |\nStereoSet | ,\nTrustGPT |\nUnQover |\nWinoBias |\nWinoBias+ |\nWinoGender |\nWinoQueer |\nAbout\nNo description, website, or topics provided.\nResources\nReadme\nActivity\nStars\n131 stars\nWatchers\n4 watching\nForks\n10 forks\nReport repository\nReleases\nNo releases published\nPackages 0\nNo packages published\nLanguages\nPython 93.8%\nShell 6.2%\nFooter\n¬© 2025 GitHub, Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStatus\nDocs\nContact\nManage cookies\nDo not share my personal information\n\nYou can‚Äôt perform that action at this time.", "similarity": 95}, {"title": "Cognitive Bias in Large Language Models: Implications for Research and Practice", "url": "https://ai.nejm.org/doi/full/10.1056/AIe2400961", "txt": "Editorial\nCognitive Bias in Large Language Models: Implications for Research and Practice\nAuthor: Laura Zwaan, Ph.D. Author Info & Affiliations\nPublished November 27, 2024\nNEJM AI 2024;1(12)\nDOI: 10.1056/AIe2400961\nVOL. 1 NO. 12\nCopyright ¬© 2024\nPermissions\nFor permission requests, please contact NEJM Reprints at reprints@nejm.org\nContents\nAbstract\nNotes\nSupplementary Material\nInformation & Authors\nMetrics & Citations\nGet Access\nReferences\nMedia\nTables\nShare\n\nAbstract\nThe use of large language models (LLMs) such as ChatGPT in clinical settings is growing, but concerns about their susceptibility to cognitive biases persist. Wang and Redelmeier‚Äôs study reveals that LLMs are prone to biases, raising important questions about their role in medical decision-making. To prevent errors in decision-making with LLMs, it is recommended that clinicians aim to critically engage with LLMs (e.g. refuting their hypotheses rather than looking for confirmation) researchers should focus on identifying and evaluating collaborative strategies between AI and human decision-making. Furthermore, research on context-specific implementation is important. We need to ensure that AI complements, rather than replicates, human cognitive processes. (Funded by the Netherlands Organisation for Health Research and Development.)\nUnderstand the impact of AI on your practice. Get the tools to evaluate the risks.\nMonthly & Annual Options\nSubscribe Now\nNotes\nSupported by a grant (ZonMw Vidi grant number: 09150172210015 to Dr. Zwaan) from the Netherlands Organisation for Health Research and Development.\nDisclosure forms provided by the authors are available with the full text of this article.\nSupplementary Material\nDisclosure Forms (aie2400961_disclosures.pdf)\nDownload\n67.97 KB\nInformation & Authors\nInformationAuthors\nInformation\nPublished In\nNEJM AI\nVolume 1 ‚Ä¢ Number 12 ‚Ä¢ November 27, 2024\nCopyright\nCopyright ¬© 2024 Massachusetts Medical Society.\nFor personal use only. Any commercial reuse of NEJM Group content requires permission.\nHistory\nSubmitted : October 1, 2024\nAccepted : October 7, 2024\nPublished online : November 27, 2024\nPublished in issue : November 27, 2024\nTopics\nMedical Statistics General\nAuthors\nAffiliationsExpand All\nLaura Zwaan, Ph.D.\nAssociate Professor, Erasmus Medical Center ‚Äî Institute of Medical Education Research, Rotterdam, Netherlands\nNotes\nDr. Zwaan can be contacted at l.zwaan@erasmusmc.nl.\nMetrics & Citations\nMetricsCitations1\nMetrics\nAltmetrics\nCitations\nExport citation\nSelect the format you want to export the citation of this publication.\nFormat* Please Select\nRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorks\nPlease select an item in the list\nDirect Import\nExport citation\nCited by\n1. Taro Shimizu,\nSatoshi Watanuki,\nYukinori Harada,\nRen Kawamura,\nMasayuki Amano,\nSho Isoda,\nKotaro Kunitomo,\nMamoru Komatsu,\nTaiju Miyagami,\nKosuke Ishizuka,\nShintaro Kosaka,\nMasaru Kurihara,\nPioneering diagnosis in Asia: advancing clinical reasoning expertise through the lens of 3M, Diagnosis, (2025).\nCrossref\nLoading...\nMedia\nFiguresOther\nFigures\nOther\nTables\nShare\nShare\nCONTENT LINK\nCopy Link\nCopied!\nCopying failed.\nShare\nFacebookX (formerly Twitter)LinkedInemailBluesky\nGet Access\nGet Access\nReferences\nReferences\nEvaluating clinical AI: Be empowered with the evidence you need.\nSubscribe Now\nOpen in viewer\nClose figure viewer\nGo to\nGo to\nShow all references\nRequest permissionsExpand All\nCollapse\nExpand Table\nAuthors Info & Affiliations\nNow Reading:\nCognitive Bias in Large Language Models: Implications for Research and Practice\nShare\nShare on Facebook\nShare on X (formerly Twitter)\nShare on LinkedIn\nShare on email\nShare on Bluesky\n\nNEXT ARTICLEA New Foundation Model for Multimodal Ophthalmic Images: Advancing Disease Detection and PredictionNext\nFiguresTablesClose figure viewer\nReference 1\nclose pop-up\nEvaluating clinical AI: Be empowered with the evidence you need.\nMonthly & Annual Options\nSubscribe Now\nBROWSE\nResearch\nReview\nCase Study\nCommentary\nPerspective\nOther\nCurrent Issue\nIssue Index\nRESOURCES\nEvents\nAuthor Center\nSubmit a Manuscript\nSubscribers\nInstitutional Administrators\nMedia\nBusiness Model\nAgents\nPermissions & Licensing\nReprints\nNEJM CareerCenter\nABOUT US\nAbout NEJM AI\nNEJM Group\nEditors & Publishers\nEditorial Policies\nFAQs\nHelp\nContact Us\nSUBSCRIPTIONS\nSubscribe\nRenew\nActivate Subscription\nCreate Account\nManage Account\nPay Bill\nInstitutional Sales\nInstitution Administration Center\nSTAY CONNECTED\nContact Us\nCreate Account\nEmail Alerts\nRemote Access\nNEJM CareerCenter\nFOLLOW US\nX (formerly Twitter)\nLinkedIn\nYouTube\nBluesky\nJOURNALS\nThe New England Journal of Medicine\nNEJM Catalyst Innovations in Care Delivery\nNEJM Evidence\nNEJM AI\nNEJM AI is a product of NEJM Group, a division of the Massachusetts Medical Society.\nCopyright ¬© 2025 Massachusetts Medical Society. All rights reserved, including those for text and data mining, AI training, and similar technologies. ISSN 2836-9386.\nCopyright\nTerms\nPrivacy Policy\nNEJM Group logo\nBack to top\nSign In\nSign In\nRemember Me\nForgot your password?\nSign In\nDon't have an account?\nCreate Account Subscribe\nCreate Account Subscribe\nSign In\nSign In\nRemember Me\nForgot your password?\nSign In\nDon't have an account?\nCreate Account Subscribe\nForgot Password\nEnter the email address associated with your account then click Continue. We will email you a link to a page where you can easily create a new password.\nCreate New Password\nWe've sent an email with instructions to create a new password. Your existing password has not been changed.\nCLOSE\nTo reset your password, enter a new password twice and click the 'Reset Password' button.\nYour password has been reset. You will need to sign in again using your new password to access site content and features.\nThe link that you followed to reset your password has expired.\nEnter the email address associated with your account then click Continue. We will email you a link to a page where you can easily create a new password.\nIf the address matches an existing account, you will receive an email with instructions to reset your password.", "similarity": 90}, {"title": "Understanding Bias and Fairness in Large Language Models (LLMs)", "url": "https://uniathena.com/understanding-bias-fairness-large-language-models-llms", "txt": "Understanding Bias and Fairness in Large Language Models (LLMs)\nAuthor: neha mondal\n5 MINS READ\n0\n276\n21 January, 2025\nAuthor: neha mondal\n5 MINS READ\n0\n276\n21 January, 2025\nAs Artificial Intelligence (AI) becomes increasingly embedded in our daily lives, AI's concepts of bias and fairness, especially in large language models (LLMs), are becoming more critical. These tools, which power applications like chatbots, translation services, and content generation, can transform industries. But they also pose significant challenges. Let‚Äôs explore how Bias manifests in LLMs and what fairness means.\nWhat is Bias and Fairness in LLMs?\nBias in AI refers to systematic errors or tendencies that a model might make regarding its predictions that unfairly favour one group and disadvantage another. In Machine Learning, this Bias usually comes through the data used to develop models, the design of algorithms, or the interpretations of the outputs. The biased dataset that reflects stereotypes may make the LLM reproduce the stereotype in its responses.\nFairness in AI is about having the system treat all users equally and avoiding the perpetuation of societal inequities. In other words, models need to be designed with respect for diversity and deliver good results regardless of the user's background.\nHow does Bias Arise in LLMs?\nLLMs, like GPT or any other tool, are trained on massive datasets drawn from the internet, books, and other digital repositories. Such datasets often contain historical, cultural, and societal biases. Below are some of the key areas in which bias creeps into LLMs:\nData Bias : The training data may overrepresent or underrepresent certain groups, thereby leading to skewed outputs. For instance, if a dataset consists of predominantly Western-centric texts, the model may end up generating responses that favour Western perspectives.\nRead More\nCOMMENTS(0)\nComment\nRich Text Editor\nParagraph\nBoldItalicUnderlineLinkBulleted List\nNumbered ListNumbered List\nText alignment\nUndoRedo\nAbout text formats\nName\nEmail\nOur Popular Insights\nOur Popular Insights\nWhy Free Learning is the Best Way to Learn New Skills?\nRead More 5 mins read\nWhy DataOps is the Future of Data Engineering and Analytics\nRead More 5 mins read\nDedication & Excellence: Celebrating Our Students of the Month\nRead More 1 mins read\nWhy Free Learning is the Best Way to Learn New Skills?\nRead more 5 mins read\nWhy DataOps is the Future of Data Engineering and Analytics\nRead more 5 mins read\nDedication & Excellence: Celebrating Our Students of the Month\nRead more 1 mins read\n‚Äπ‚Ä∫\nOur Popular Courses\n$14000\nRating\nDoctorate of Business Administration\nUniversidad Catolica De Murcia (UCAM), Spain\nDuration:\n2 - 3 Years\nLearn More\n$17500*\nRating\nIntegrated Doctorate of Business Administration\nUniversidad Catolica De Murcia (UCAM), Spain\nDuration:\n2.5 - 3.5 Years\nLearn More\n$4600*\nRating\nMaster of Business Administration\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4200*\nRating\nMBA in Operations & Project Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Supply Chain and Logistics Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4950*\nRating\nMaster in Data Science\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Engineering Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Procurement and Contract Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Public Health\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\nPrevious Next\n1 2 3\nOur Popular Courses\n$14000\nRating\nDoctorate of Business Administration\nUniversidad Catolica De Murcia (UCAM), Spain\nDuration:\n2 - 3 Years\nLearn More\n$17500*\nRating\nIntegrated Doctorate of Business Administration\nUniversidad Catolica De Murcia (UCAM), Spain\nDuration:\n2.5 - 3.5 Years\nLearn More\n$4600*\nRating\nMaster of Business Administration\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4200*\nRating\nMBA in Operations & Project Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Supply Chain and Logistics Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4950*\nRating\nMaster in Data Science\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Engineering Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Procurement and Contract Management\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n$4600*\nRating\nMaster in Public Health\nGuglielmo Marconi University, Italy\nDuration:\n9 - 24 Months\nLearn More\n‚Äπ‚Ä∫\nGet in Touch\nYour Name\nEmail address\nMobile Number\nCountry Code\n(+65)\nAfghanistan (+93)Albania (+355)Algeria (+213)American Samoa (+1)Andorra (+376)Angola (+244)Anguilla (+1)Antigua & Barbuda (+1)Argentina (+54)Armenia (+374)Aruba (+297)Ascension Island (+247)Australia (+61)Austria (+43)Azerbaijan (+994)Bahamas (+1)Bahrain (+973)Bangladesh (+880)Barbados (+1)Belarus (+375)Belgium (+32)Belize (+501)Benin (+229)Bermuda (+1)Bhutan (+975)Bolivia (+591)Bosnia & Herzegovina (+387)Botswana (+267)Brazil (+55)British Indian Ocean Territory (+246)British Virgin Islands (+1)Brunei (+673)Bulgaria (+359)Burkina Faso (+226)Burundi (+257)Cambodia (+855)Cameroon (+237)Canada (+1)Cape Verde (+238)Caribbean Netherlands (+599)Cayman Islands (+1)Central African Republic (+236)Chad (+235)Chile (+56)China (+86)Christmas Island (+61)Cocos (Keeling) Islands (+61)Colombia (+57)Comoros (+269)Congo - Brazzaville (+242)Congo - Kinshasa (+243)Cook Islands (+682)Costa Rica (+506)Croatia (+385)Cuba (+53)Cura√ßao (+599)Cyprus (+357)Czechia (+420)C√¥te d‚ÄôIvoire (+225)Denmark (+45)Djibouti (+253)Dominica (+1)Dominican Republic (+1)Ecuador (+593)Egypt (+20)El Salvador (+503)Equatorial Guinea (+240)Eritrea (+291)Estonia (+372)Eswatini (+268)Ethiopia (+251)Falkland Islands (+500)Faroe Islands (+298)Fiji (+679)Finland (+358)France (+33)French Guiana (+594)French Polynesia (+689)Gabon (+241)Gambia (+220)Georgia (+995)Germany (+49)Ghana (+233)Gibraltar (+350)Greece (+30)Greenland (+299)Grenada (+1)Guadeloupe (+590)Guam (+1)Guatemala (+502)Guernsey (+44)Guinea (+224)Guinea-Bissau (+245)Guyana (+592)Haiti (+509)Honduras (+504)Hong Kong SAR China (+852)Hungary (+36)Iceland (+354)India (+91)Indonesia (+62)Iran (+98)Iraq (+964)Ireland (+353)Isle of Man (+44)Israel (+972)Italy (+39)Jamaica (+1)Japan (+81)Jersey (+44)Jordan (+962)Kazakhstan (+7)Kenya (+254)Kiribati (+686)Kosovo (+383)Kuwait (+965)Kyrgyzstan (+996)Laos (+856)Latvia (+371)Lebanon (+961)Lesotho (+266)Liberia (+231)Libya (+218)Liechtenstein (+423)Lithuania (+370)Luxembourg (+352)Macao SAR China (+853)Madagascar (+261)Malawi (+265)Malaysia (+60)Maldives (+960)Mali (+223)Malta (+356)Marshall Islands (+692)Martinique (+596)Mauritania (+222)Mauritius (+230)Mayotte (+262)Mexico (+52)Micronesia (+691)Moldova (+373)Monaco (+377)Mongolia (+976)Montenegro (+382)Montserrat (+1)Morocco (+212)Mozambique (+258)Myanmar (Burma) (+95)Namibia (+264)Nauru (+674)Nepal (+977)Netherlands (+31)New Caledonia (+687)New Zealand (+64)Nicaragua (+505)Niger (+227)Nigeria (+234)Niue (+683)Norfolk Island (+672)North Korea (+850)North Macedonia (+389)Northern Mariana Islands (+1)Norway (+47)Oman (+968)Pakistan (+92)Palau (+680)Palestinian Territories (+970)Panama (+507)Papua New Guinea (+675)Paraguay (+595)Peru (+51)Philippines (+63)Poland (+48)Portugal (+351)Puerto Rico (+1)Qatar (+974)Romania (+40)Russia (+7)Rwanda (+250)R√©union (+262)Samoa (+685)San Marino (+378)Saudi Arabia (+966)Senegal (+221)Serbia (+381)Seychelles (+248)Sierra Leone (+232)Singapore (+65)Sint Maarten (+1)Slovakia (+421)Slovenia (+386)Solomon Islands (+677)Somalia (+252)South Africa (+27)South Korea (+82)South Sudan (+211)Spain (+34)Sri Lanka (+94)St. Barth√©lemy (+590)St. Helena (+290)St. Kitts & Nevis (+1)St. Lucia (+1)St. Martin (+590)St. Pierre & Miquelon (+508)St. Vincent & Grenadines (+1)Sudan (+249)Suriname (+597)Svalbard & Jan Mayen (+47)Sweden (+46)Switzerland (+41)Syria (+963)S√£o Tom√© & Pr√≠ncipe (+239)Taiwan (+886)Tajikistan (+992)Tanzania (+255)Thailand (+66)Timor-Leste (+670)Togo (+228)Tokelau (+690)Tonga (+676)Trinidad & Tobago (+1)Tristan da Cunha (+290)Tunisia (+216)Turkmenistan (+993)Turks & Caicos Islands (+1)Tuvalu (+688)T√ºrkiye (+90)U.S. Virgin Islands (+1)Uganda (+256)Ukraine (+380)United Arab Emirates (+971)United Kingdom (+44)United States (+1)Uruguay (+598)Uzbekistan (+998)Vanuatu (+678)Vatican City (+39)Venezuela (+58)Vietnam (+84)Wallis & Futuna (+681)Western Sahara (+212)Yemen (+967)Zambia (+260)Zimbabwe (+263)√Öland Islands (+358)\n(+65)\nPhone number\nCourse Category What are you looking for?Bachelor'sDoctorateMBAMastersPostgraduate CertificationsPostgraduate DiplomasUndergraduate Diplomas\nCAPTCHA\nGet new captcha!\nWhat code is in the image?\nEnter the characters shown in the image.\nI agree with Terms & Conditions\nIt‚Äôs Time to Start Investing In Yourself\nJoin Now\nMost Popular Online Specialization\nMaster of International Business Administration\nMaster of Business Administration\nMBA in General Management- FastTrack\nMaster in Innovation and Entrepreneurship\nMBA-Family Business Management\nMaster in Procurement and Contracts Management\nExtended Diploma in Business Analytics (SCQF Level 11)\nDiploma in Supply Chain and Logistics Management (SCQF Level 11)\nStrategic Human Resource Management Practitioner\nExecutive MBA in Business Analytics\nMaster in Data Science\nMaster in Engineering Management\nTrending Online\nDoctorate of Business Administration\nIntegrated Doctorate of Business Administration\nPostgraduate Certificate in Finance for Next Generation Managers\nMaster of Business Administration- General Management (Fast Track)\nPostgraduate Certificate in Socio-Economic and Legal Framework\nPostgraduate Certificate in Business Sustainability\nCertified Manager\nSupply Chain Management Practitioner\nMSc Digital Marketing and e-Business\nMSc Accounting and Finance (CIMA Gateway)\nExecutive MBA\nMaster in Supply Chain and Logistics Management\nTop Universities Online Certificates\nPostgraduate Certificate in International Marketing Management\nPostgraduate Certificate In International Human Resource Management\nPostgraduate Certificate in Strategic Management\nPostgraduate Certificate in Procurement & Contracts Management\nPostgraduate Certificate in Business Analytics\nPostgraduate Certificate in Strategic Supply Chain & Logistics Management\nPostgraduate Certificate in Human Resource and Leadership\nProject Management Practitioner\nPostgraduate Certificate in Supply Chain Design & Implementation\nPostgraduate Certificate in Management Accounting and Finance\nPostgraduate Certificate in Digital Marketing\nPostgraduate Certificate in General Management\nAccredited Online Degree Program\nMBA - Digital Transformation\nMBA - Family Business Management\nMBA - Marketing Management\nMBA in Quality Management\nMBA - Business Intelligence & Data Analytics\nMBA in Operations & Project Management\nMBA in Energy Management\nMBA In Construction & Safety Management\nMaster in Organisational Leadership\nMaster in Public Health\nMaster in Construction Management\nBachelor of Arts in Business Administration\n\nUniAthena is an Ed-Tech, offering flexible, affordable learning solutions, including Free-Learning Upskilling Courses and Academic Programs in partnerships with accredited and globally renowned universities and professional qualification bodies.\n\nAbout Us\nPrivacy Policy\nPolicies & Procedures\nOther Fee & Charges\nContact Us\nTerms & Conditions\nFAQ Glossary\nRefer And Earn\nApply Now\nFree Certificate\nAccreditation & Partnerships\nPartner With Us\nSubscribe to our Newsletter and Webinars\nName\nTerms & Conditions\nSubscribe To Webinar Series\nCAPTCHA\nGet new captcha!\nWhat code is in the image?\nEnter the characters shown in the image.\nDo you have any questions ?\nFeel free to send us your questions or request a free consultation\nSend a message\n‚ÄúThe more that you read, the more things you will know, the more that you learn, the more places you‚Äôll go.‚Äù\nDr. Seuss\nUK\nAthena Global Education Magdalen Centre, Robert Robinson Avenue, Oxford, OX4 4GA, UK Phone : 01865 784299\nMIDDLE EAST\nAthena Global Education FZE Block L-03, First Floor, P O Box 519265, Sharjah Publishing City, Free Zone, Sharjah, UAE Phone : +971 65 31 2511\nINDIA\nUniathena Private Limited 9A,Midas Tower Phase 1 Hinjewadi Rajiv Gandhi Infotech Park Pune-411057 Phone: +91 9145665544\nAll Copyrights Reserved @ Athena Global Education 2021-2025\nCopy link\n‚úì\nThanks for sharing!\nFind any service\nAddToAny\nMore‚Ä¶\nThis website uses cookies to ensure you get the best experience on our website\nMore info\nAccept No, thanks", "similarity": 75}]}
